{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import openml\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import torch as th\n",
    "from torch.utils.data import random_split, TensorDataset\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "from tabpfn import TabPFNClassifier\n",
    "\n",
    "from tab_pfn.networks import TabPFN\n",
    "from tab_pfn.metrics import ConfusionMeter\n",
    "from tab_pfn.metrics import AccuracyMeter\n",
    "from tab_pfn.networks import pad_features"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "benchmark = openml.study.get_suite('OpenML-CC18')\n",
    "tasks = openml.tasks.list_tasks(task_id=benchmark.tasks, output_format=\"dataframe\")\n",
    "\n",
    "retained_datasets = []\n",
    "\n",
    "for _, row in tqdm(list(tasks.iterrows())):\n",
    "    try:\n",
    "        datasets = openml.tasks.get_task(row[\"tid\"]).get_dataset()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(row[\"tid\"])\n",
    "        continue\n",
    "    \n",
    "    if row[\"NumberOfInstances\"] > 2000:\n",
    "        continue\n",
    "    if row[\"NumberOfNumericFeatures\"] > 100:\n",
    "        continue\n",
    "    if datasets.qualities[\"NumberOfClasses\"] > 10:\n",
    "        continue\n",
    "    \n",
    "    retained_datasets.append(openml.tasks.get_task(row[\"tid\"]).get_dataset())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "feae5940a59c6899",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "len(retained_datasets)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a62a33067d4948c1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tab_pfn = TabPFN(100, 10, 256, 512, 1024, 4, 12)\n",
    "tab_pfn.load_state_dict(th.load(\"../resources/model_48127.pt\"))\n",
    "tab_pfn.eval()\n",
    "\n",
    "tab_pfn_clf = TabPFNClassifier(device=\"cuda\")\n",
    "\n",
    "scores = {}\n",
    "\n",
    "for dataset in tqdm(retained_datasets):\n",
    "    x, y, _, _ = dataset.get_data(target=dataset.default_target_attribute)\n",
    "    \n",
    "    classes = {\n",
    "        c: i for i, c in enumerate(y.unique())  \n",
    "    }\n",
    "    \n",
    "    conf_meter = ConfusionMeter(len(classes), None)\n",
    "    acc_meter = AccuracyMeter(None)\n",
    "    \n",
    "    y = y.apply(lambda c: classes[c])\n",
    "    \n",
    "    # ohe\n",
    "    for c in x.columns:\n",
    "        if not is_numeric_dtype(x[c]):\n",
    "            ohe = pd.get_dummies(x[c], prefix=c, prefix_sep=\"_\", dtype=float)\n",
    "            x = x.drop(c, axis=1).join(ohe)\n",
    "            if len(x.columns) > 100:\n",
    "                break\n",
    "    \n",
    "    if len(x.columns) > 100:\n",
    "        continue\n",
    "\n",
    "    with th.no_grad():\n",
    "        x = th.tensor(x.to_numpy())\n",
    "        y = th.tensor(y.to_numpy())\n",
    "        \n",
    "        x, y = x[x.size(0) % 2:], y[y.size(0) % 2:]\n",
    "        th_dataset = TensorDataset(x, y)\n",
    "        data_train, data_test = random_split(th_dataset, [0.5, 0.5])\n",
    "        \n",
    "        x_train, y_train = zip(*[data_train[i] for i in range(len(data_train))])\n",
    "        x_train, y_train = pad_features(th.stack(x_train, dim=0).to(th.float), 100), th.stack(y_train, dim=0).to(th.long)\n",
    "        \n",
    "        x_test, y_test = zip(*[data_test[i] for i in range(len(data_test))])\n",
    "        x_test, y_test = pad_features(th.stack(x_test, dim=0).to(th.float), 100), th.stack(y_test, dim=0).to(th.long)\n",
    "        \n",
    "        out = tab_pfn(x_train[None], y_train[None], x_test[None])[0]\n",
    "        #tab_pfn_clf.fit(x_train, y_train)\n",
    "        #out = th.tensor(tab_pfn_clf.predict_proba(x_test))\n",
    "        \n",
    "        loss = F.cross_entropy(out, y_test, reduction='mean').cpu().item()\n",
    "        conf_meter.add(out, y_test)\n",
    "        acc_meter.add(out, y_test)\n",
    "        \n",
    "        scores[dataset.name] = {\n",
    "            \"loss\": loss,\n",
    "            \"conf_meter\": conf_meter,\n",
    "            \"acc_meter\": acc_meter,\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ec9d2e79c411cde",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for n, score in scores.items():\n",
    "    conf_meter = score[\"conf_meter\"]\n",
    "    print(f\"{n} : loss={score['loss']:.4f}, acc={score['acc_meter'].accuracy()}, prec={conf_meter.precision().mean().cpu().item():.4f} | rec={conf_meter.recall().mean().cpu().item():.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9598cee841b010a3",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "3 : loss=0.1690, acc=0.9507 | rec=0.9491\n",
    "11 : loss=0.2526, acc=0.8263 | rec=0.8025\n",
    "14 : loss=0.5841, acc=0.7704 | rec=0.7747\n",
    "15 : loss=nan, acc=0.3295 | rec=0.5000\n",
    "16 : loss=0.2647, acc=0.9336 | rec=0.9341\n",
    "18 : loss=0.7649, acc=0.6760 | rec=0.6736\n",
    "22 : loss=0.4107, acc=0.8285 | rec=0.8313\n",
    "23 : loss=1.0401, acc=0.1406 | rec=0.3333\n",
    "29 : loss=nan, acc=0.2377 | rec=0.5000\n",
    "31 : loss=0.5340, acc=0.6851 | rec=0.5948\n",
    "37 : loss=0.4828, acc=0.7703 | rec=0.7518\n",
    "46 : loss=0.9109, acc=0.5742 | rec=0.5267\n",
    "50 : loss=0.4127, acc=0.7752 | rec=0.7244\n",
    "54 : loss=0.6417, acc=0.6987 | rec=0.7033\n",
    "188 : loss=nan, acc=0.0652 | rec=0.2000\n",
    "38 : loss=nan, acc=0.4706 | rec=0.5000\n",
    "458 : loss=0.0191, acc=0.9958 | rec=0.9964\n",
    "469 : loss=1.8085, acc=0.0327 | rec=0.1667\n",
    "1049 : loss=0.2422, acc=0.9570 | rec=0.6187\n",
    "1050 : loss=0.2954, acc=0.4501 | rec=0.5000\n",
    "1063 : loss=0.4099, acc=0.7803 | rec=0.6860\n",
    "1067 : loss=0.3358, acc=0.5825 | rec=0.5066\n",
    "1068 : loss=0.2535, acc=0.7101 | rec=0.5101\n",
    "1510 : loss=0.0973, acc=0.9519 | rec=0.9586\n",
    "1494 : loss=0.3405, acc=0.8471 | rec=0.8348\n",
    "1480 : loss=0.5533, acc=0.5745 | rec=0.5225\n",
    "1487 : loss=0.1464, acc=0.8084 | rec=0.5150\n",
    "1462 : loss=0.0508, acc=0.9887 | rec=0.9849\n",
    "1464 : loss=0.5115, acc=0.8881 | rec=0.5174\n",
    "6332 : loss=nan, acc=0.2185 | rec=0.5000\n",
    "23381 : loss=0.7787, acc=0.5707 | rec=0.5681\n",
    "40966 : loss=nan, acc=0.0188 | rec=0.1250\n",
    "40982 : loss=0.8137, acc=0.7151 | rec=0.6522\n",
    "40994 : loss=0.2078, acc=0.8597 | rec=0.6902\n",
    "40975 : loss=0.2396, acc=0.8264 | rec=0.6853\n",
    "40984 : loss=0.3858, acc=0.8246 | rec=0.8291\n",
    "40978 : loss=0.2861, acc=0.9171 | rec=0.7535\n",
    "40670 : loss=1.0561, acc=0.1668 | rec=0.3333"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94bcb1e3a844e760"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "conf_meters = [s[\"conf_meter\"] for _, s in scores.items()]\n",
    "acc_meters = [s[\"acc_meter\"] for _, s in scores.items()]\n",
    "cross_entropies = [s[\"loss\"] for _, s in scores.items()]\n",
    "\n",
    "precision = sum(conf_meter.precision().mean().item() for conf_meter in conf_meters) / len(conf_meters)\n",
    "recall = sum(conf_meter.recall().mean().item() for conf_meter in conf_meters) / len(conf_meters)\n",
    "acc = sum(acc_meter.accuracy() for acc_meter in acc_meters) / len(acc_meters)\n",
    "\n",
    "\n",
    "cross_entropies_without_nan = list(map(lambda c: math.log(10) if math.isnan(c) else c, cross_entropies))\n",
    "\n",
    "print(f\"precision = {precision}, recall = {recall}\")\n",
    "print(\"cross entropy:\", sum(cross_entropies_without_nan) / len(cross_entropies_without_nan))\n",
    "print(\"acc:\", acc)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18a13be171922d13",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "precision = 0.6425595247235737, recall = 0.6251027662503091\n",
    "cross entropy: 0.7661994506457874"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93c48ffc15d05530"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Author code :\n",
    "\n",
    "precision = 0.801273051649332, recall = 0.77424192322152\n",
    "cross entropy: 0.8216769727213042\n",
    "acc: 0.837828790733493"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3236daae243f64"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e75c488da0f61b6b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
